%%=============================================================================
%% Methodologie
%%=============================================================================

\chapter{\IfLanguageName{dutch}{Methodologie}{Methodology}}
\label{ch:methodologie}

\section{Plan van aanpak}


De meest effectieve benadering voor het uitvoeren van dit onderzoek omvat een reeks stappen. Allereerst wordt de data-acquisitie uitgevoerd om relevante gegevens te verzamelen. Het is van essentieel belang om te bepalen waar de meest geschikte bronnen zijn om de benodigde data te verkrijgen en welke methoden kunnen worden toegepast.

Na het succesvol verzamelen van de data, volgt het proces van datafiltering, waarbij onnodige rijen en kolommen worden verwijderd en de aanwezigheid van bruikbare informatie wordt geëvalueerd.

Zodra alle vereiste data beschikbaar is, wordt de data voorbereid voor het gedeelte van machinaal leren met behulp van geschikte Python-bibliotheken. Daarnaast wordt er een analyse uitgevoerd op de data om mogelijk interessante bevindingen en conclusies te identificeren.

Vervolgens wordt het best mogelijke AI-model getraind. Verschillende modellen zullen worden geëvalueerd en getest om het meest optimale model te identificeren. Dit model zal verder worden verfijnd om een nauwkeurigheidsscore van 95\% te behalen.

Ten slotte zal er een kleine applicatie worden ontwikkeld waarin gebruikers de mogelijkheid krijgen om, op basis van het getrainde model, voorspellingen te doen over schaakpartijen tussen twee grootmeesters. Om een applicatie te genereren, kan de code lokaal worden uitgevoerd of kan het commando 'pyinstaller --onefile bestandsnaam.py' worden gebruikt. Dit commando resulteert in de creatie van een uitvoerbaar bestand dat de code in de vorm van een applicatie laat draaien.

Door deze methodische aanpak kan het onderzoek gestructureerd worden uitgevoerd, waarbij de focus ligt op het verkrijgen van betrouwbare data, het analyseren ervan, het trainen van een nauwkeurig AI-model en het ontwikkelen van een gebruiksvriendelijke applicatie. 

\section{Ophalen data}

\subsection{Data extrusion}

In het kader van deze bachelorproef werd als eerste stap gekeken naar de meest geschikte bron voor het verkrijgen van de benodigde data. Voordat bepaald wordt waar de data kan verkrijgbaar is, is het essentieel om de vraag te stellen: Welke specifieke data is nodig?

Het doel van dit onderzoek is om een model te trainen dat de uitkomst van toekomstige schaakpartijen kan voorspellen op basis van gespeelde partijen. Aangezien het onderzoek zich uitsluitend richt op grootmeesters, zijn alleen partijen nodig die gespeeld zijn tussen twee grootmeesters. Bovendien is het belangrijk om te weten welke spelers bij welke partij betrokken waren, welke rating ze op dat moment hadden en met welke kleur ze elk speelden.

Er zijn verschillende databases die partijen bevatten die aan de gestelde criteria voldoen. Websites en applicaties zoals ChessBase, Chess.com, Lichess en vele anderen bieden de meeste benodigde data voor dit onderzoek. De keuze is echter gevallen op het verkrijgen van data van de officiële FIDE-website, omdat FIDE de belangrijkste coördinator is van professioneel schaken. Bovendien bleek uit de functionaliteit van de FIDE-zoekmachine dat deze kan filteren op grootmeesters, actieve spelers en partijlengte. Deze veelzijdige filteringsopties waren overtuigend genoeg om deze bron te verkiezen.

Voor het verkrijgen van de data waren er drie mogelijke benaderingen:

\begin{itemize}
    \item Het gebruik van een door een internationale FIDE-scheidsrechter ontwikkelde Python-parser \autocite{Larreategi}
    \item Een webscraper van de GitHub-repository van \textcite{Alves2020}
    \item Het ontwikkelen van een eigen API en scraper.
\end{itemize}

Het oorspronkelijke plan was om eerst te onderzoeken of één van de eerste twee opties haalbaar was voordat werd overwogen om zelf een API te ontwikkelen. Er is gestart met het verkennen van de Python-scraper van Mikel Larreategi. Al snel werd echter een probleem ontdekt. De scraper was oorspronkelijk bedoeld om alleen toernooipartijen op te halen. Daarnaast was de verzamelde informatie niet bruikbaar voor het beoogde doel. Belangrijke spelersgegevens, zoals hun ELO-rating, ontbraken, terwijl overbodige informatie, zoals de betrokken scheidsrechters bij elke partij, wel aanwezig was. Het werd al snel duidelijk dat deze benadering niet geschikt was om te volgen.

Op het eerste gezicht leek de tweede optie niet erg veelbelovend, aangezien de API al geruime tijd geleden was ontwikkeld en mogelijk niet meer up-to-date was. Bovendien was de volledige implementatie in JavaScript en was er geen ingebouwde Python-scraper die met de API kon werken. Nader onderzoek onthulde echter een kleine scraper in de GitHub-repository die profielgegevens kon ophalen van elke geregistreerde speler op de FIDE-website, mits de speler-ID bekend was. De speler-ID is niet gebaseerd op de rating of ranglijst van de speler, maar eerder op het tijdstip van toevoeging aan de database of een willekeurige toewijzing.

Een belangrijk voordeel van deze repository is de MIT-licentie die eraan verbonden is. Deze licentie luidt als volgt: "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the 'Software'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so." Het enige vereiste voor het gebruik van de software is het opnemen van de bovenstaande auteursrechtvermelding en toestemmingsverklaring in alle kopieën of substantiële delen van de software.

\subsection{Doelstelling}

De beschikbare scraper in de GitHub-repository biedt de mogelijkheid om de profielgegevens van individuele spelers op te halen. Zodra deze informatie wordt verkregen, is er echter alleen toegang tot de profielgegevens van de spelers, terwijl onze interesse voornamelijk uitgaat naar de volledige partijgeschiedenis van elke speler. Deze partijgeschiedenis omvat alle gespeelde partijen gedurende de gehele carrière van de betreffende spelers.

Deze partijgeschiedenis vormt de basis voor de gewenste dataset. Het verkrijgen van de volledige partijgeschiedenis voor elke individuele speler zou ons in staat stellen om alle benodigde gegevens te verwerven voor ons onderzoek.

\subsection{Spelerdata}

De API is geïmplementeerd in JavaScript, maar gezien mijn ervaring met die taal, ligt mijn expertise niet bij het ontwikkelen van een scraper in JavaScript. Mijn voorkeur gaat eerder uit naar het gebruik van Python voor dit doeleinde, aangezien ik vertrouwd ben met het schrijven van scrapers in deze programmeertaal.

Het plan is dan ook om de scraper te implementeren in Python, met behulp van de bibliotheek Beautiful Soup 4. Beautiful Soup is een Python-bibliotheek waarmee gegevens rechtstreeks in het UTF-8-formaat kunnen worden geëxtraheerd van een website. Deze keuze biedt mij de mogelijkheid om gebruik te maken van de functionaliteiten en flexibiliteit die Python biedt bij het ontwikkelen van de scraper.\autocite{Richardson}

\begin{lstlisting}[language=Python]
    import re
    import requests
    from bs4 import BeautifulSoup
    
    # get fide ids
    print("Getting fide ids")
    fide_ids_url = f"https://ratings.fide.com/incl_search_l.php?search=&search_rating={rating}&search_country=all&search_title={title}&search_other_title=all&search_year=undefined&search_low=0&search_high=3500&search_inactive=on&search_exrated=off&search_radio=rating&search_bday_start=all&search_bday_end=all&search_radio=rating&search_asc=descending&search_gender=undefined&simple=0"
    fide_ids_html = requests.get(fide_ids_url, headers={'X-Requested-With': 'XMLHttpRequest'}).text
    soup = BeautifulSoup(fide_ids_html, 'html.parser')
    anchors = soup.find_all('a')
    hrefs = [a.get('href') for a in anchors]
    pattern = re.compile(r"/profile/(\d+)")
    fide_ids = [pattern.search(href).group(1) for href in hrefs if pattern.search(href)]
\end{lstlisting}

Hierbij worden alle ID's opgehaald van profielen naar keuze met behulp van beautifulsoup. Het starten van dit script zorgt ervoor dat een applicatie opstart dat vraagt aan de gebruiker om te kiezen uit standaard, rapid of blitz (de drie tijdsmodules). Nadat deze keuze is gemaakt, vraagt het programma ook welke filter je wilt kiezen. Dit is filter dat een lijst geeft met alle mogelijke titels waaronder de titel grootmeesteer. De data die je kan ophalen is dus flexibel maar voor dit project's doeleinden, worden enkel de partijen van de grootmeesters opgehaald. Zo zijn alle ID's van alle actieve grootmeesters bemachtigd.

\begin{lstlisting}[language=Python]
    import re
    import subprocess
    import pandas as pd
    from tqdm import tqdm
    
    # get info of players
    print("Getting info of players")
    
    players = []
    process = subprocess.Popen("fide-ratings-scraper api", stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
    for i in tqdm(range(len(fide_ids))):
        fide_id = fide_ids[i]
        response = requests.get(f"http://localhost:3000/player/{fide_id}/info")
        player = response.json()
        player["fide_id"] = fide_id
        player["name"] = player["name"].strip()
        players.append(player)
    process.kill()
    
    # print out data to csv
    df = pd.DataFrame(players)
    df.to_csv(f"fide_players_{rating}_{title}.csv", index=False)
\end{lstlisting}

In dit proces worden met behulp van Beautiful Soup alle ID's van geselecteerde profielen opgehaald. Bij het starten van dit script wordt een applicatie geïnitialiseerd, waarin de gebruiker wordt gevraagd om een keuze te maken uit de standaard-, rapid- of blitzmodus (de drie beschikbare tijdsmodules). Nadat deze keuze is gemaakt, wordt de gebruiker ook gevraagd welk filter hij of zij wenst toe te passen.

Deze filter stelt een lijst samen met alle mogelijke titels, waaronder de titel "grootmeester". Het is belangrijk op te merken dat de op te halen data flexibel is. Voor de specifieke doeleinden van dit onderzoek wordt de focus gelegd op het verkrijgen van de ID's van actieve grootmeesters. Hierdoor worden alle ID's van de huidige grootmeesters verzameld, wat van cruciaal belang is voor het onderzoek.\autocite{NumFocus}

\subsection{Historiek}

Op het eerste gezicht lijkt de tussenstap voor het ophalen van spelergegevens misschien overbodig, aangezien er gebruik gemaakt wordt van een ID om profielinformatie op te halen. Is het echter mogelijk om ook de partijgeschiedenis op te halen? Zeker, maar hier doet zich een probleem voor: in de partijgeschiedenis van een speler wordt alleen de naam van de tegenstander vermeld, niet de bijbehorende ID. Het was essentieel om de ID's van elke speler aan een specifieke naam te koppelen, zodat er kan gecontroleerd wordt of de tegenstander op de lijst van grootmeesters staat. De code is zo ontworpen dat deze schaalbaar is voor het geval iemand alle wedstrijden van een groep spelers wil ophalen zonder dat er een filter wordt toegepast op een reeds bestaand CSV-bestand.

\begin{lstlisting}[language=Python]
    import re
    import pandas as pd
    
    #get games of player
    player_games = []

    def output_player_games(player_games):
        df = pd.DataFrame(player_games)
        match = re.search(r'fide_players_(.+)_([^\.]+)\.csv', players_file_name)
        if not opponents_from_csv:
            df.to_csv(f"fide_games_{match.group(1)}_{match.group(2)}_all.csv", index=False)
        else:
            df.to_csv(f"fide_games_{match.group(1)}_{match.group(2)}_oppfromcsv.csv", index=False)
\end{lstlisting}

Binnen de applicatie krijgt een gebruiker de mogelijkheid om aan te geven of er al dan niet moet worden gefilterd op naam. Indien de eindgebruiker ervoor kiest om geen filtering toe te passen tijdens het scrapproces, zal het programma de if-statement doorlopen. Daarentegen, indien de eindgebruiker wel filtering wenst, zal de scraper de else-statement volgen. Deze aanpak stelt ons in staat om meerdere CSV-bestanden te genereren voor verschillende doeleinden. Voor dit specifieke project, is er ervoor gekozen om te filteren op de naam. Dit besluit is genomen omdat er enkel interesse is in partijen waarbij een grootmeester tegen een andere grootmeester speelt.

\section{Filtering data}

\subsection{Verstandhouding}

Er zijn drie csv-bestanden gemaakt vanuit het scraping-process. Één bestand voor elke tijdscategorie (standaard, blitz en rapid). De inhoud van deze csv-bestanden houden alle partijen van grootmeesters die tegen een andere grootmeester gespeeld hebben in. Deze bevatten een aantal kolommen. De kolommen zijn als volgt:

\begin{itemize}
    \item Datum
    \item Naam speler
    \item Naam tegenstander
    \item w => uitkomst van de partij
    \item n => som uitkomst van beide spelers
    \item chg => changerate (zal later op dieper ingegaan worden)
    \item k => katalysator/factor
    \item kchg => k vermenigvuligd met chg (elo gain/loss)
    \item Evenement
    \item Locatie
    \item Land van afkomst speler
    \item Titel speler
    \item Rating tegenstander
    \item Land van afkomst tegenstander 
    \item Titel tegenstander
    \item Kleur => kleur van speler
\end{itemize}

Technisch gezien hebben we alle data dat we nodig hebben hier. Echter vertonen er snel drie problemen.

\subsection{Problematiek}

Probleem 1: Er zijn heel wat overbodige kolommen met data dat weinig tot geen meerwaarde brengen. Perfecte voorbeelden hiervan is de kolom 'Titel speler' en 'Titel tegenstander'. De eerder vermelde scrapers die opgebouwd zijn met filters, zorgen ervoor dat er enkel mensen met de titel grootmeester in de dataset zitten. 

Probleem 2: De rating van de tegenstander staat op elke lijn vermeldt. Dit is een dynamisch gegeven want de ELO-rating van elke speler wordt pas geüpdate de dag nadat die persoon zijn of haar partijen heeft gespeeld. Deze traag-veranderde waarde is correct voor elke grootmeester maar het probleem is dat er geen kolom is met de ELO-rating van de speler zelf. Zo is het mogelijk om een vergelijking te doen van skill-niveau tussen de twee spelers.

Probleem 3: Alle partijen staan dubbel in het bestand. Als speler x tegen speler y een partij heeft gespeeld op datum z, dan zal dit ook zo staan in de historiek van speler x. Dezelfde logica is ook toepasbaar voor speler y. Dit wil zeggen dat er een record in het csv-bestand zal zijn waarbij speler y tegen speler x heeft gespeeld op dag z. Dit kan een probleem vormen bij het trainen van een model omdat je dubbele data zal trainen en hiermee de nauwkeurigheid mee kan affecteren. 

\subsection{Oplossingen}

Het eerste probleem aanpakken is vrij simpel. De applicatie zal de overbodige kolommen verwijderen. Het moeilijkste aan dit process, is kiezen welke kolommen er weg moeten.
\begin{lstlisting}[language=Python]
    import pandas as pd
    df_games = pd.read_csv(games_file_name)
    df_games = df_games.drop('k', axis=1)
    df_games = df_games.drop('chg', axis=1)
    df_games = df_games.drop('n', axis=1)
    df_games = df_games.drop('event', axis=1)
    df_games = df_games.drop('location', axis=1)
    df_games = df_games.drop('country', axis=1)
    df_games = df_games.drop('opponent_country', axis=1)
    df_games = df_games.drop('opponent_title', axis=1)
    df_games = df_games.drop('player_title', axis=1)
\end{lstlisting}

De kolom k wordt gedroppt met de reden dat deze kolom dezelfde waarde bevatte voor alle grootmeesters. In het regulatiedocument gemaakt door de \textcite{FIDE2021}, staat de k in als de ontwikkelingscoëfficiënt. Rating-systemen behandelen mensen die een tijd niet meer hebben gespeeld, of nieuw zijn in schaken als 'volatile'. Dit wil zeggen dat hun rating-increment meer wordt beïnvloedt. Het belangrijkste detail aan deze statistiek is dat deze waarde tien is voor alle grootmeesters. Dit komt omdat eenmaal een persoon een ELO-rating van 2400 behaalt, deze factor automatisch naar 10 zakt, ookal zou dit twintig, dertig of veertig zijn. Dit beïnvloedt de ELO-gain en ELO-loss na een match maar is voor onze dataset niet van toepassing en heeft geen effect. Op vlak van schaalbaarheid is dit wel anders, indien later een onderzoek wil gevoerd worden over nationale meesters of FIDE-meesters, dan kan dit wel een invloedsfactor zijn voor het trainen van een model. 

De volgende keuze was moeilijk te maken. De kolom chg en kchg bevatten technisch dezelfde data enkel is het verscil dat de waardes in kchg exact vermenigvuldigd zijn met de factor tien ten op zichte van chg. Doordat dit twee kolommen zijn die dezelfde data bevatten, moet er één wegvallen. De keuze dat chg wegvalt is omdat kchg ook staat als ELO-gain/ELO-loss voor die partij. Maar waarvoor precies staat chg?

Chg staat voor de kans dat de speler wint/of verliest. In de literatuurstudie is hierop dieper ingegaan en samen met de 'volatitity-factor' die al is meegerekend in Kchg, is het beter dat voor dit onderzoek de kolom chg weg wordt laten gevallen.

Kolom n is de totaalwaarde van de uitkomst van de spelers. Met dat schaken een nul-som spel is (een spel waarbij het totaaal aantal winnende spellen gelijk is and het totaal aantal verliezende spellen), betekent dat deze waarde altijd hetzelfde zal zijn voor elke rij. De totaal waarde is één doordat winnen de winnende speler één punt oplevert en de verliezende speler nul, en bij gelijkspel krijgen beide spelers een halve punt. 

De locatie en evenement zouden normaal geen rol moeten spelen op de data. In tegenstelling tot voetbal of andere sporten waarbij supporters psylogische aanmoediging, prestatiedruk of stress kunnen geven, is dit bij schaken erchter niet zo. De zalen zijn afgesloten en ookal kan locatie een verschil maken op vlak van slaapritme, de meeste professionele schakers zorgen ervoor dat dit hun niet affecteert. Schaken is ook een mentaal spel waarbij het hoofd koel houden heel belangrijk is, is het ook dat externe factoren iemand niet kan afleiden. 

Het land van afkomst voor beide spelers zou ook geen effect mogen hebben op de dataset. Het is niet zoals rensport waarbij mensen vanuit Kenia, die trainen in de hoge bergen, daardoor een beter lichamelijk eco-systeem ontwikkelen dat dit ook van toepassing is bij schaken. Iemand zijn performantie is gebaseerd op wat de persoon en geleerd heeft over het spel. Dit is ook om te vermijden dat bijvoorbeeld een land over-gerepresenteerd wordt of onder-gerepresenteerd wordt. Een goed voorbeeld hiervan is dat Magnus Carlsen afkomstig is uit Noorwegen en er nog twintig grootmeesters uit Noorwegen komen (van de 1800+). Met dat hij veel meer spellen wint tegen ook veel sterkere tegenstanders, zou het AI model hierop misschien ook dingen kunnen van opnemen en verkeerde conclusies uit trekken.

Als laatste zijn de spelers' titels ook niet meer van belang voor de dataset met dat overal de titel grootmeester (g) is toegekend en deze dataset ook gefilterd werd op enkel grootmeesters.

Het tweede probleem heeft een duidelijke oplossingslijn die elegant kan toegepast worden, echter de implementatie om dit te vinden was zeer moeilijk. 

\begin{lstlisting}[language=Python]
    df_games_rating = df_games[['date','player_name','opponent_name','opponent_rating']]
    def fail(row):
        df_result = df_games_rating[(df_games_rating.date == row.date) & (df_games_rating.player_name == row.opponent_name) & (df_games_rating.opponent_name == row.player_name)]
        return str(df_result.opponent_rating.values[0]) if not df_result.empty else ''
    df_games['current_rating'] = df_games[['date','player_name','opponent_name','opponent_rating']].apply(fail, axis=1)
\end{lstlisting}

Het doel van de code is dat we een nieuwe kolom 'current rating' aanmaken waarbij de rating van de speler kan getoond worden dat die persoon had op die dag tegen zijn tegenstander. Het programma zal de huidige ELO-rating van de speler als 'df result' opslaan. Deze waarde wordt opgehaald door de record te zoeken waarbij de datum gelijk is aan de originele record, en waarbij 'naam speler' en 'naam tegenstander' omgewisseld zijn. Als het programma dan deze record heeft gevonden, zal het de rating van de tegenstander teruggeven (wat gelijk is aan de rating van de speler waarna we het op zoek waren). Dan zal er een nieuwe kolom aangemaakt worden waarbij deze waarde zal aan toegevoegd worden. Het argument axis=1 geeft aan dat de sorteerbewerking moet worden toegepast op elke rij. 

Het probleem kon aangepakt worden met een lamba-fucntie. Dit is getest geweest maar is nooit gelukt geweest en heeft altijd voor problemen gezorgd. De manier hoe dit nu beschreven staat, werkt.

Het laatste probleem kan op een gelijkaardige aanpak opgelost worden.

\begin{lstlisting}[language=Python]
    df_sorted = df_games.copy()
    df_sorted[['date','player_name', 'opponent_name']] = df_sorted[['date','player_name', 'opponent_name']].apply(sorted, axis=1, result_type='expand')
    name_not_switched = (df_sorted['player_name'] == df_games['player_name'])
    df_games = df_sorted.loc[name_not_switched]
\end{lstlisting}

Er wordt een kopie gemaakt van de oorspronkelijke DataFrame df games en toegewezen aan df sorted. Dit zorgt ervoor dat we de oorspronkelijke DataFrame behouden terwijl we wijzigingen aanbrengen in df sorted. In de volgende regel worden de kolommen 'date', 'player name' en 'opponent name' van df sorted opnieuw toegewezen door de sorted-functie toe te passen op elke rij. Dit sorteert de waarden in elke rij van deze kolommen in oplopende volgorde. Het argument result type='expand' zorgt ervoor dat de resultaten van de sorteerbewerking worden uitgebreid naar meerdere kolommen. Daarachter wordt een booleaanse serie name not switched gemaakt waarin wordt gecontroleerd of de waarden in de kolom 'player name' van df sorted overeenkomen met de waarden in de kolom 'player name' van de oorspronkelijke DataFrame df games. En in de laatste rij wordt de DataFrame df games  bijgewerkt door alleen de rijen te behouden waarin de waarden in de kolom 'player name' overeenkomen met de waarden in de kolom 'player name' van df sorted. Dit resulteert in een gefilterde DataFrame waarin alleen de rijen staan waarin de speler niet van positie is gewisseld met de tegenstander, m.a.w. de dubbele record eruit gehaald.

Deze applicatie heeft ervoor gezorgd dat er een nieuw csv-bestand bestaat met alle nodige data om het model te trainen.

\section{Voorbereiden data}

