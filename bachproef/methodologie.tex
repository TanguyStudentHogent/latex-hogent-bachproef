%%=============================================================================
%% Methodologie
%%=============================================================================

\chapter{\IfLanguageName{dutch}{Methodologie}{Methodology}}
\label{ch:methodologie}

\section{Plan van aanpak}


De meest effectieve benadering voor het uitvoeren van dit onderzoek omvat een reeks stappen. Allereerst wordt de data-acquisitie uitgevoerd om relevante gegevens te verzamelen. Het is van essentieel belang om te bepalen waar de meest geschikte bronnen zijn om de benodigde data te verkrijgen en welke methoden kunnen worden toegepast.

Na het succesvol verzamelen van de data, volgt het proces van datafiltering, waarbij onnodige rijen en kolommen worden verwijderd en de aanwezigheid van bruikbare informatie wordt geëvalueerd.

Zodra alle vereiste data beschikbaar is, wordt de data voorbereid voor het gedeelte van machinaal leren met behulp van geschikte Python-bibliotheken. Daarnaast wordt er een analyse uitgevoerd op de data om mogelijk interessante bevindingen en conclusies te identificeren.

Vervolgens wordt het best mogelijke AI-model getraind. Verschillende modellen zullen worden geëvalueerd en getest om het meest optimale model te identificeren. Dit model zal verder worden verfijnd om een nauwkeurigheidsscore van 95\% te behalen.

Ten slotte zal er een kleine applicatie worden ontwikkeld waarin gebruikers de mogelijkheid krijgen om, op basis van het getrainde model, voorspellingen te doen over schaakpartijen tussen twee grootmeesters. Om een applicatie te genereren, kan de code lokaal worden uitgevoerd of kan het commando 'pyinstaller --onefile bestandsnaam.py' worden gebruikt. Dit commando resulteert in de creatie van een uitvoerbaar bestand dat de code in de vorm van een applicatie laat draaien.

Door deze methodische aanpak kan het onderzoek gestructureerd worden uitgevoerd, waarbij de focus ligt op het verkrijgen van betrouwbare data, het analyseren ervan, het trainen van een nauwkeurig AI-model en het ontwikkelen van een gebruiksvriendelijke applicatie. 

\section{Ophalen data}

\subsection{Data extrusion}

In het kader van deze bachelorproef werd als eerste stap gekeken naar de meest geschikte bron voor het verkrijgen van de benodigde data. Voordat bepaald wordt waar de data kan verkrijgbaar is, is het essentieel om de vraag te stellen: Welke specifieke data is nodig?

Het doel van dit onderzoek is om een model te trainen dat de uitkomst van toekomstige schaakpartijen kan voorspellen op basis van gespeelde partijen. Aangezien het onderzoek zich uitsluitend richt op grootmeesters, zijn alleen partijen nodig die gespeeld zijn tussen twee grootmeesters. Bovendien is het belangrijk om te weten welke spelers bij welke partij betrokken waren, welke rating ze op dat moment hadden en met welke kleur ze elk speelden.

Er zijn verschillende databases die partijen bevatten die aan de gestelde criteria voldoen. Websites en applicaties zoals ChessBase, Chess.com, Lichess en vele anderen bieden de meeste benodigde data voor dit onderzoek. De keuze is echter gevallen op het verkrijgen van data van de officiële FIDE-website, omdat FIDE de belangrijkste coördinator is van professioneel schaken. Bovendien bleek uit de functionaliteit van de FIDE-zoekmachine dat deze kan filteren op grootmeesters, actieve spelers en partijlengte. Deze veelzijdige filteringsopties waren overtuigend genoeg om deze bron te verkiezen.

Voor het verkrijgen van de data waren er drie mogelijke benaderingen:

\begin{itemize}
    \item Het gebruik van een door een internationale FIDE-scheidsrechter ontwikkelde Python-parser \autocite{Larreategi}
    \item Een webscraper van de GitHub-repository van \textcite{Alves2020}
    \item Het ontwikkelen van een eigen API en scraper.
\end{itemize}

Het oorspronkelijke plan was om eerst te onderzoeken of één van de eerste twee opties haalbaar was voordat werd overwogen om zelf een API te ontwikkelen. Er is gestart met het verkennen van de Python-scraper van Mikel Larreategi. Al snel werd echter een probleem ontdekt. De scraper was oorspronkelijk bedoeld om alleen toernooipartijen op te halen. Daarnaast was de verzamelde informatie niet bruikbaar voor het beoogde doel. Belangrijke spelersgegevens, zoals hun ELO-rating, ontbraken, terwijl overbodige informatie, zoals de betrokken scheidsrechters bij elke partij, wel aanwezig was. Het werd al snel duidelijk dat deze benadering niet geschikt was om te volgen.

Op het eerste gezicht leek de tweede optie niet erg veelbelovend, aangezien de API al geruime tijd geleden was ontwikkeld en mogelijk niet meer up-to-date was. Bovendien was de volledige implementatie in JavaScript en was er geen ingebouwde Python-scraper die met de API kon werken. Nader onderzoek onthulde echter een kleine scraper in de GitHub-repository die profielgegevens kon ophalen van elke geregistreerde speler op de FIDE-website, mits de speler-ID bekend was. De speler-ID is niet gebaseerd op de rating of ranglijst van de speler, maar eerder op het tijdstip van toevoeging aan de database of een willekeurige toewijzing.

Een belangrijk voordeel van deze repository is de MIT-licentie die eraan verbonden is. Deze licentie luidt als volgt: "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the 'Software'), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so." Het enige vereiste voor het gebruik van de software is het opnemen van de bovenstaande auteursrechtvermelding en toestemmingsverklaring in alle kopieën of substantiële delen van de software.

\subsection{Doelstelling}

De beschikbare scraper in de GitHub-repository biedt de mogelijkheid om de profielgegevens van individuele spelers op te halen. Zodra deze informatie wordt verkregen, is er echter alleen toegang tot de profielgegevens van de spelers, terwijl onze interesse voornamelijk uitgaat naar de volledige partijgeschiedenis van elke speler. Deze partijgeschiedenis omvat alle gespeelde partijen gedurende de gehele carrière van de betreffende spelers.

Deze partijgeschiedenis vormt de basis voor de gewenste dataset. Het verkrijgen van de volledige partijgeschiedenis voor elke individuele speler zou ons in staat stellen om alle benodigde gegevens te verwerven voor ons onderzoek.

\subsection{Spelerdata}

De API is geïmplementeerd in JavaScript, maar gezien mijn ervaring met die taal, ligt mijn expertise niet bij het ontwikkelen van een scraper in JavaScript. Mijn voorkeur gaat eerder uit naar het gebruik van Python voor dit doeleinde, aangezien ik vertrouwd ben met het schrijven van scrapers in deze programmeertaal.

Het plan is dan ook om de scraper te implementeren in Python, met behulp van de bibliotheek Beautiful Soup 4. Beautiful Soup is een Python-bibliotheek waarmee gegevens rechtstreeks in het UTF-8-formaat kunnen worden geëxtraheerd van een website. Deze keuze biedt mij de mogelijkheid om gebruik te maken van de functionaliteiten en flexibiliteit die Python biedt bij het ontwikkelen van de scraper.\autocite{Richardson}

\begin{lstlisting}[language=Python]
    import re
    import requests
    from bs4 import BeautifulSoup
    
    # get fide ids
    print("Getting fide ids")
    fide_ids_url = f"https://ratings.fide.com/incl_search_l.php?search=&search_rating={rating}&search_country=all&search_title={title}&search_other_title=all&search_year=undefined&search_low=0&search_high=3500&search_inactive=on&search_exrated=off&search_radio=rating&search_bday_start=all&search_bday_end=all&search_radio=rating&search_asc=descending&search_gender=undefined&simple=0"
    fide_ids_html = requests.get(fide_ids_url, headers={'X-Requested-With': 'XMLHttpRequest'}).text
    soup = BeautifulSoup(fide_ids_html, 'html.parser')
    anchors = soup.find_all('a')
    hrefs = [a.get('href') for a in anchors]
    pattern = re.compile(r"/profile/(\d+)")
    fide_ids = [pattern.search(href).group(1) for href in hrefs if pattern.search(href)]
\end{lstlisting}

Hierbij worden alle ID's opgehaald van profielen naar keuze met behulp van beautifulsoup. Het starten van dit script zorgt ervoor dat een applicatie opstart dat vraagt aan de gebruiker om te kiezen uit standaard, rapid of blitz (de drie tijdsmodules). Nadat deze keuze is gemaakt, vraagt het programma ook welke filter je wilt kiezen. Dit is filter dat een lijst geeft met alle mogelijke titels waaronder de titel grootmeesteer. De data die je kan ophalen is dus flexibel maar voor dit project's doeleinden, worden enkel de partijen van de grootmeesters opgehaald. Zo zijn alle ID's van alle actieve grootmeesters bemachtigd.

\begin{lstlisting}[language=Python]
    import re
    import subprocess
    import pandas as pd
    from tqdm import tqdm
    
    # get info of players
    print("Getting info of players")
    
    players = []
    process = subprocess.Popen("fide-ratings-scraper api", stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
    for i in tqdm(range(len(fide_ids))):
        fide_id = fide_ids[i]
        response = requests.get(f"http://localhost:3000/player/{fide_id}/info")
        player = response.json()
        player["fide_id"] = fide_id
        player["name"] = player["name"].strip()
        players.append(player)
    process.kill()
    
    # print out data to csv
    df = pd.DataFrame(players)
    df.to_csv(f"fide_players_{rating}_{title}.csv", index=False)
\end{lstlisting}

In dit proces worden met behulp van Beautiful Soup alle ID's van geselecteerde profielen opgehaald. Bij het starten van dit script wordt een applicatie geïnitialiseerd, waarin de gebruiker wordt gevraagd om een keuze te maken uit de standaard-, rapid- of blitzmodus (de drie beschikbare tijdsmodules). Nadat deze keuze is gemaakt, wordt de gebruiker ook gevraagd welk filter hij of zij wenst toe te passen.

Deze filter stelt een lijst samen met alle mogelijke titels, waaronder de titel "grootmeester". Het is belangrijk op te merken dat de op te halen data flexibel is. Voor de specifieke doeleinden van dit onderzoek wordt de focus gelegd op het verkrijgen van de ID's van actieve grootmeesters. Hierdoor worden alle ID's van de huidige grootmeesters verzameld, wat van cruciaal belang is voor het onderzoek.\autocite{NumFocus}

\subsection{Historiek}

Op het eerste gezicht lijkt de tussenstap voor het ophalen van spelergegevens misschien overbodig, aangezien er gebruik gemaakt wordt van een ID om profielinformatie op te halen. Is het echter mogelijk om ook de partijgeschiedenis op te halen? Zeker, maar hier doet zich een probleem voor: in de partijgeschiedenis van een speler wordt alleen de naam van de tegenstander vermeld, niet de bijbehorende ID. Het was essentieel om de ID's van elke speler aan een specifieke naam te koppelen, zodat er kan gecontroleerd wordt of de tegenstander op de lijst van grootmeesters staat. De code is zo ontworpen dat deze schaalbaar is voor het geval iemand alle wedstrijden van een groep spelers wil ophalen zonder dat er een filter wordt toegepast op een reeds bestaand CSV-bestand.

\begin{lstlisting}[language=Python]
    import re
    import pandas as pd
    
    #get games of player
    player_games = []

    def output_player_games(player_games):
        df = pd.DataFrame(player_games)
        match = re.search(r'fide_players_(.+)_([^\.]+)\.csv', players_file_name)
        if not opponents_from_csv:
            df.to_csv(f"fide_games_{match.group(1)}_{match.group(2)}_all.csv", index=False)
        else:
            df.to_csv(f"fide_games_{match.group(1)}_{match.group(2)}_oppfromcsv.csv", index=False)
\end{lstlisting}

Binnen de applicatie krijgt een gebruiker de mogelijkheid om aan te geven of er al dan niet moet worden gefilterd op naam. Indien de eindgebruiker ervoor kiest om geen filtering toe te passen tijdens het scrapproces, zal het programma de if-statement doorlopen. Daarentegen, indien de eindgebruiker wel filtering wenst, zal de scraper de else-statement volgen. Deze aanpak stelt ons in staat om meerdere CSV-bestanden te genereren voor verschillende doeleinden. Voor dit specifieke project, is er ervoor gekozen om te filteren op de naam. Dit besluit is genomen omdat er enkel interesse is in partijen waarbij een grootmeester tegen een andere grootmeester speelt.

\section{Filtering data}

\subsection{Verstandhouding}

Tijdens het scraping-proces zijn drie csv-bestanden gegenereerd, elk bestand vertegenwoordigt een specifieke tijdscategorie (standaard, blitz en rapid). Deze csv-bestanden bevatten uitgebreide informatie over partijen gespeeld tussen grootmeesters. Ze omvatten verschillende kolommen die als volgt zijn gestructureerd:

\begin{itemize}
    \item Datum
    \item Naam speler
    \item Naam tegenstander
    \item w => uitkomst van de partij
    \item n => som uitkomst van beide spelers
    \item chg => changerate (zal later op dieper ingegaan worden)
    \item k => katalysator/factor
    \item kchg => k vermenigvuligd met chg (elo gain/loss)
    \item Evenement
    \item Locatie
    \item Land van afkomst speler
    \item Titel speler
    \item Rating tegenstander
    \item Land van afkomst tegenstander 
    \item Titel tegenstander
    \item Kleur => kleur van speler
\end{itemize}

Hoewel technisch gezien alle benodigde gegevens verzameld zijn, doen zich echter snel drie problemen voor.

\subsection{Problematiek}

Probleem 1: Er zijn verschillende overbodige kolommen met data die weinig tot geen toegevoegde waarde bieden. Dit geldt met name voor de kolommen 'Titel speler' en 'Titel tegenstander'. Aangezien de eerder vermelde scrapers zijn ingesteld met filters om alleen grootmeesters op te nemen in de dataset, bevatten deze kolommen redundantie.

Probleem 2: De rating van de tegenstander wordt vermeld op elke regel. Deze rating is een dynamische waarde, aangezien de ELO-rating van elke speler pas wordt bijgewerkt op de dag nadat ze hun partijen hebben gespeeld. Hoewel deze langzaam veranderende waarde correct is voor elke grootmeester, ontbreekt er een kolom met de ELO-rating van de speler zelf. Hierdoor is het niet mogelijk om een directe vergelijking te maken van het vaardigheidsniveau tussen de twee spelers.

Probleem 3: Er is een duplicatie van partijen in het bestand. Als speler X tegen speler Y een partij heeft gespeeld op datum Z, dan zal deze informatie ook voorkomen in de historie van speler Y. Op dezelfde manier zal er een record in het CSV-bestand zijn waarbij speler Y tegen speler X heeft gespeeld op dezelfde dag Z. Dit leidt tot dubbele gegevens en kan een probleem vormen bij het trainen van een model, omdat dit de nauwkeurigheid van het model kan beïnvloeden.

\subsection{Oplossingen}

Het eerste probleem kan op een relatief eenvoudige manier worden aangepakt. De applicatie zal de overbodige kolommen verwijderen. Het uitdagendste aspect van dit proces is echter het maken van de juiste keuze welke kolommen verwijderd moeten worden. Dit vereist zorgvuldige afweging en het bepalen van welke kolommen weinig tot geen toegevoegde waarde bieden voor de analyse of het beoogde doel van de gegevensverwerking. Het is belangrijk om rekening te houden met de specifieke vereisten en doelstellingen van het project om een weloverwogen beslissing te nemen over welke kolommen behouden moeten blijven en welke kunnen worden verwijderd.

\begin{lstlisting}[language=Python]
    import pandas as pd
    df_games = pd.read_csv(games_file_name)
    df_games = df_games.drop('k', axis=1)
    df_games = df_games.drop('chg', axis=1)
    df_games = df_games.drop('n', axis=1)
    df_games = df_games.drop('event', axis=1)
    df_games = df_games.drop('location', axis=1)
    df_games = df_games.drop('country', axis=1)
    df_games = df_games.drop('opponent_country', axis=1)
    df_games = df_games.drop('opponent_title', axis=1)
    df_games = df_games.drop('player_title', axis=1)
\end{lstlisting}

De kolom 'k' wordt verwijderd omdat deze kolom dezelfde waarde bevat voor alle grootmeesters. Volgens het regulatiedocument opgesteld door de \textcite{FIDE2021}, vertegenwoordigt 'k' de ontwikkelingscoëfficiënt. Rating-systemen behandelen spelers die lange tijd niet hebben gespeeld of nieuw zijn in het schaken als 'volatile'. Dit betekent dat hun rating-verandering sterker wordt beïnvloed. Het belangrijke detail van deze statistiek is dat de waarde van 'k' voor alle grootmeesters gelijk is aan tien. Dit komt doordat zodra een speler een ELO-rating van 2400 bereikt, deze factor automatisch wordt vastgesteld op tien, ongeacht of het oorspronkelijk twintig, dertig of veertig was. Hoewel dit van invloed is op de ELO-winst en ELO-verlies na een partij, is dit voor onze dataset niet relevant en heeft het geen effect. Het kan echter wel een belangrijke factor zijn voor schaalbaarheid, mocht er in de toekomst onderzoek worden gedaan naar nationale meesters of FIDE-meesters, waarbij het trainen van een model beïnvloed kan worden door deze factor.


De kolom 'chg' vertegenwoordigt de kans dat een speler wint of verliest. In de literatuurstudie is dit aspect nader onderzocht en gezien de aanwezigheid van de 'volatility-factor' die al wordt meegenomen in 'kchg', is het voor dit onderzoek beter om de kolom 'chg' te verwijderen. Aangezien beide kolommen technisch gezien dezelfde data bevatten, met alleen het verschil dat de waarden in 'kchg' vermenigvuldigd zijn met een factor van tien ten opzichte van 'chg', is het logischer om 'chg' te laten vallen en 'kchg' te behouden. Hierdoor kunnen we ons concentreren op de ELO-winst/-verlies in 'kchg' voor elke partij, wat meer relevant is voor het onderzoek.

De kolom 'n' vertegenwoordigt de totale waarde van de uitkomst van de spelers. Aangezien schaken een nul-som spel is, wat betekent dat het totale aantal gewonnen spellen gelijk is aan het totale aantal verloren spellen, zal deze waarde altijd hetzelfde zijn voor elke rij in het dataset. De totale waarde is in dit geval één, omdat een winnende speler één punt ontvangt, de verliezende speler nul punten krijgt, en bij een gelijkspel ontvangen beide spelers een half punt.

De kolommen 'locatie' en 'evenement' zouden normaal gesproken geen significante invloed moeten hebben op de data. In tegenstelling tot sporten zoals voetbal of andere sporten waarbij supporters psychologische aanmoediging, prestatiedruk of stress kunnen veroorzaken, is dit bij schaken niet het geval. Schaakzalen zijn meestal afgesloten om afleiding te minimaliseren, en hoewel de locatie mogelijk een verschil kan maken in slaapritme, zorgen de meeste professionele schakers ervoor dat externe factoren hen niet beïnvloeden. Schaken is een mentaal spel waarbij het behouden van een heldere geest cruciaal is, en daarom worden externe factoren die iemand kunnen afleiden doorgaans geminimaliseerd.

Het land van herkomst voor beide spelers zou naar verwachting geen invloed moeten hebben op de dataset. In tegenstelling tot bijvoorbeeld paardenraces, waarbij atleten uit bepaalde regio's mogelijk fysieke voordelen hebben als gevolg van training op grote hoogte, geldt dit niet voor schaken. De prestaties van een schaker zijn voornamelijk gebaseerd op zijn of haar kennis en vaardigheden in het spel. Het vermijden van landgebonden factoren helpt ook om te voorkomen dat bepaalde landen oververtegenwoordigd of ondervertegenwoordigd worden in de dataset. Een goed voorbeeld hiervan is Magnus Carlsen, die afkomstig is uit Noorwegen en daar vele sterke tegenstanders heeft. Als het AI-model gevoelig zou zijn voor het land van herkomst, zou het mogelijk verkeerde conclusies kunnen trekken op basis van deze specifieke situatie.

Ten slotte zijn de titels van de spelers ook niet langer relevant voor de dataset, omdat alle spelers in de dataset de titel "grootmeester" (g) hebben en de dataset al gefilterd is op alleen grootmeesters. Daarom kunnen de kolommen met de spelers' titels veilig worden verwijderd, aangezien ze geen extra waarde toevoegen aan de analyse.

Het tweede probleem kan op een elegante manier worden opgelost met een duidelijke aanpak, maar de implementatie van deze oplossing was uitdagend.

\begin{lstlisting}[language=Python]
    df_games_rating = df_games[['date','player_name','opponent_name','opponent_rating']]
    def fail(row):
        df_result = df_games_rating[(df_games_rating.date == row.date) & (df_games_rating.player_name == row.opponent_name) & (df_games_rating.opponent_name == row.player_name)]
        return str(df_result.opponent_rating.values[0]) if not df_result.empty else ''
    df_games['current_rating'] = df_games[['date','player_name','opponent_name','opponent_rating']].apply(fail, axis=1)
\end{lstlisting}

Het doel van de code is dat we een nieuwe kolom 'current rating' aanmaken waarbij de rating van de speler kan getoond worden dat die persoon had op die dag tegen zijn tegenstander. Het programma zal de huidige ELO-rating van de speler als 'df result' opslaan. Deze waarde wordt opgehaald door de record te zoeken waarbij de datum gelijk is aan de originele record, en waarbij 'naam speler' en 'naam tegenstander' omgewisseld zijn. Als het programma dan deze record heeft gevonden, zal het de rating van de tegenstander teruggeven (wat gelijk is aan de rating van de speler waarna we het op zoek waren). Dan zal er een nieuwe kolom aangemaakt worden waarbij deze waarde zal aan toegevoegd worden. Het argument axis=1 geeft aan dat de sorteerbewerking moet worden toegepast op elke rij. 

Het probleem werd aangepakt met een lambda-functie, maar helaas leverde dit geen succes op en veroorzaakte het problemen. De huidige beschreven aanpak heeft daarentegen wel gewerkt.

Het laatste probleem kan op een vergelijkbare manier worden opgelost.

\begin{lstlisting}[language=Python]
    df_sorted = df_games.copy()
    df_sorted[['date','player_name', 'opponent_name']] = df_sorted[['date','player_name', 'opponent_name']].apply(sorted, axis=1, result_type='expand')
    name_not_switched = (df_sorted['player_name'] == df_games['player_name'])
    df_games = df_sorted.loc[name_not_switched]
\end{lstlisting}

De oorspronkelijke DataFrame 'df games' wordt gekopieerd naar 'df sorted' om de oorspronkelijke gegevens te behouden tijdens het uitvoeren van wijzigingen. Vervolgens worden de kolommen 'date', 'player name' en 'opponent name' van 'df sorted' opnieuw toegewezen door de 'sorted'-functie toe te passen op elke rij. Hierdoor worden de waarden in elke rij van deze kolommen gesorteerd in oplopende volgorde.

Met het argument 'result type='expand'' wordt ervoor gezorgd dat de resultaten van de sorteerbewerking worden uitgebreid naar meerdere kolommen. Daarna wordt een booleaanse serie 'name not switched' gemaakt waarin wordt gecontroleerd of de waarden in de kolom 'player name' van 'df sorted' overeenkomen met de waarden in de kolom 'player name' van de oorspronkelijke DataFrame 'df games'.

Ten slotte wordt de DataFrame 'df games' bijgewerkt door alleen de rijen te behouden waarin de waarden in de kolom 'player name' overeenkomen met de waarden in de kolom 'player name' van 'df sorted'. Hierdoor wordt een gefilterde DataFrame verkregen waarin alleen de rijen staan waarin de speler niet van positie is gewisseld met de tegenstander, oftewel de dubbele records zijn verwijderd.

Deze applicatie heeft geleid tot de creatie van een nieuw CSV-bestand dat alle benodigde gegevens bevat om het model te trainen.

\section{Voorbereiden data}

\subsection{Overzicht}

Na het verkrijgen van het nieuwe CSV-bestand is de volgende essentiële stap het uitvoeren van gegevensreiniging. Om de data op te schonen, worden mogelijke inconsistenties, fouten of lege waarden geïdentificeerd en gecorrigeerd. Dit kan bijvoorbeeld inhouden dat, ontbrekende waarden worden ingevuld op basis van andere relevante gegevensbronnen en datatypen worden gevalideerd en aangepast voor consistente verwerking. Door dit zorgvuldige proces van gegevensreiniging wordt de kwaliteit en betrouwbaarheid van de dataset gewaarborgd. Dit proces zorgt ervoor dat de dataset klaar is voor verdere analyse en modeltraining. 

Bovendien biedt de reeds beschikbare data interessante mogelijkheden voor verdere analyse. Door het toepassen van statistische methoden, visualisaties en exploratieve technieken kunnen waardevolle patronen, trends en correlaties in de data worden ontdekt. Deze inzichten kunnen helpen bij het begrijpen van de schaakspelpatronen, spelersprestaties en andere interessante aspecten van het spel. 

\subsection{Data opschonen}

Eerst wordt alle data ingelezen met behulp van de library pandas (pd.read). Eenmaal dit gedaan is, wordt gekeken of alle data correct is ingeladen en alle kolommen ook correcte waarden aantonen (data.head()). Een header laten genereren zorgt ervoor dat enkel de eerste paar rijen zien maar niet weten of alle data correct is opgevuld of er waardes ontbreken in de dataset. Ook wordt er best gekeken of alle nuttige data ook onder het juiste datatype vallen. AI modellen kunnen echter niet werken met strings, op uitzondering dat de waardes worden opgedeeld afhankelijke variabele klassen. Dit zal in de volgende fase aan bod komen. Eenmaal alle datatypes in orde blijken te zijn, kan gekeken worden naar de waardes en die dan opvullen, vervangen of verwijderen. 

Voor het volgende voorbeeld van de code, zal het csv-bestand van de standaard tijdscategorie genomen worden maar alle veranderingen aangebracht tot deze dataset kan en is toegepast op de andere twee csv-bestanden van de rapid en blitz records.

\begin{lstlisting}[language=Python]
    import pandas as pd                                 # Dataframe
    from pandas.api.types import CategoricalDtype
    data = pd.read_csv('games_std_filtered.csv',delimiter=',')
    data.head(100)
    data.info()
    ---  ------           --------------   -----  
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 185461 entries, 0 to 185460
    Data columns (total 8 columns):
    #   Column           Non-Null Count   Dtype  
    ---  ------           --------------   -----  
    0   date             185461 non-null  object 
    1   player_name      185461 non-null  object 
    2   opponent_name    185461 non-null  object 
    3   w                185461 non-null  float64
    4   k_chg            185461 non-null  float64
    5   opponent_rating  185461 non-null  int64  
    6   color            185461 non-null  object 
    7   current_rating   171774 non-null  float64
    dtypes: float64(3), int64(1), object(4)
    memory usage: 11.3+ MB
\end{lstlisting}

Zoals gezien kan worden, hebben alle kolommen die getalwaardes bevatten een datatype toegekend van een integer of een float. Dit zorgt ervoor dat er geen datatypes moeten aangepast worden en meteen kunnen overgaan naar de waarden die de kolommen bevatten.

Snel kan gezien worden dat de 'current rating' meer dan 15.000 null-waarden heeft.